{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI - CA5 - Neural Network - Phase 1 - Mohamad Taha Fakharian\n",
        "## Goal\n",
        "In this assignment, we're going to train a neural network on a handwritten persian digits dataset. Let's start!\n",
        "\n",
        "## Overall approach\n",
        "In order to train the network, we first need to read the datasets, then apply some preprocessing and after that, train the model and see the impact of hyperparameters on the accuracy of the model. Let's import some useful libraries:"
      ],
      "metadata": {
        "id": "K1YrLvD9w4tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "UMM3GDBed_Zu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we use Google Colab to train the model, we need to upload the dataset on the drive:"
      ],
      "metadata": {
        "id": "cgav8jdX0V_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McG3Oxj4esFi",
        "outputId": "3245bc5f-0469-4b55-d63b-d7de02c28871"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the dataset using pickle:"
      ],
      "metadata": {
        "id": "Yi9BqYGd0iim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('drive/My Drive/Uni/Term6/AI/dataset/data.pkl', 'rb')\n",
        "data = pickle.load(data)\n",
        "\n",
        "labels = open('drive/My Drive/Uni/Term6/AI/dataset/labels.pkl', 'rb')\n",
        "labels = pickle.load(labels)"
      ],
      "metadata": {
        "id": "kj2RqufYe3aa"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we split the dataset to train and test datasets(20 percentage of dataset goes to test and other goes to train):"
      ],
      "metadata": {
        "id": "z8HunUSN0l8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)"
      ],
      "metadata": {
        "id": "S1bw2rUau2q_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1: Data preprocessing and visualization\n",
        "Before doing anything, let's see a random image from train dataset:"
      ],
      "metadata": {
        "id": "6s4Dp7CH0yvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(random.choice(data_train))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "uedlD-iAtY31",
        "outputId": "db32ad79-35da-4dfb-a6fd-1ebdc652a5fa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the range of the values each pixel can get:"
      ],
      "metadata": {
        "id": "PZiYO-fA6LL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[0].min(), data_train[0].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hRURvlPtlpp",
        "outputId": "0c9ea2e6-1b8b-4f92-fc77-1b433f1e1b1e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 255)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since not all images in dataset have same shape, we'll resize each image to 25 * 25, using cv2 library:"
      ],
      "metadata": {
        "id": "X6NtLIha70gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [cv2.resize(img, dsize=(25, 25)) for img in data_train]\n",
        "X_test = [cv2.resize(img, dsize=(25, 25)) for img in data_test]"
      ],
      "metadata": {
        "id": "lDQuHKBwxoUu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll show an image for each number with the number itself in the train dataset:"
      ],
      "metadata": {
        "id": "EnAzdRlT8qB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 10):\n",
        "  print(\"Number = {}\".format(i))\n",
        "  index = y_train.index(i)\n",
        "  plt.imshow(X_train[index])\n",
        "  plt.show()\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bv2j3uk7zcIB",
        "outputId": "e061c5e4-17e1-4f45-c691-e1f3e304d2c0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number = 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN8ElEQVR4nO3de4xc9XnG8efp2thgTMDQbA12gnFMwBRwko2TEEShVKmDFC6qhKAIXCXVhgYkUNOkLlILkZI2ShtIlRBaEyzcioCogOBEKEBdFNNwNZbBJpCacDE2vkBNyyXF9pq3f+zQLrB7fsvOmZvf70eyZub8Xs55OebhzMw58zuOCAHY+/1GpxsA0B6EHUiCsANJEHYgCcIOJDGpnRvbx1Niqqa1c5NAkSf1VY7vOnhqcR3H9L9YSy+7Yk/l+DMb+yvH3/j1y9q963WPNtbWsE/VNH3Cp7Zzk0BR34EzKsc3n3dUcR0PfeX7tfSycei1yvHzL7qkcnztz/5+zLGm3sbbXmT7l7afsr2kmXUBaK0Jh912n6SrJX1W0nxJ59qeX1djAOrVzJF9oaSnIuLpiNgl6SZJZ9TTFoC6NRP2wyQ9P+L1psayt7E9aHu17dW7tbOJzQFoRstPvUXE0ogYiIiByZrS6s0BGEMzYd8safaI17MaywB0oWbC/rCkebbn2N5H0jmSVtTTFoC6Tfg8e0QM2b5Y0p2S+iQti4jHa+sMtXvhz04o1sz93K8qx3/147nFdRz6d/eNu6du8D8fr/53erSmc+jj8YFJ+1eO/+wfl1aOL/z9l8Yca+qimoi4Q9IdzawDQHtwbTyQBGEHkiDsQBKEHUiCsANJEHYgibb+nh2t88xff6pY84NzyueLTyrM07DqS+Ve/njGnxRr5lx2f3lFqBVHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXBRzV5iyn+NehOQt9mw87eKNSdN3Vo5vviuweI6juSCmQnbUrhJxOe+9pXK8SdfuGrMMY7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS6LqLavyxY6oLHttQXEfs3lVTN+3RN++IyvHXvhfFdaw7tj13LXnm9Oo7kkjSEft9vlgz74I1dbRT1Hfg+4o1/z2ne2LwalRfHHXwddUXLE2K18cc48gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJtl5NsHvuVG258ujKmnsHflA5ftr684rbOfDCoWLN0LMbizXt8uJV1X8NDx97c5s6qceSj/+0WPPdr55ZOX7ot+6rpZdXTj2qWLPmL6+pZVvdrqmw235W0quS9kgaioiBOpoCUL86juynRMRLNawHQAvxmR1Iotmwh6S7bD9ie9RpR20P2l5te/WeV37d5OYATFSzb+NPjIjNtt8v6W7bT0bEqpEFEbFU0lJJ2vdDh5Z/vgWgJZo6skfE5sbjdkm3SVpYR1MA6jfhsNueZnv6W88lfUbS+roaA1CvZt7G90u6zfZb6/lhRFSeYD1m2g49tPDGwmr3rRz9+XG3Fhs76fjyXUumbd1erHnzjTeKNXi3wfe9UK65tDDZxqU1NaO1da2oaTtjd7HmJ68e27LtTzjsEfG0pONr7AVAC3HqDUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS6J5bYdRo1TXlu5Z86tE/KNa89PL0OtopumLej9uyHbTOA2/sKdZceNUlxZr+79YzacdoOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiirRfVvB6hR3buqqxZsE91S32u5/9P9x9/Sy3raYdndr9WrNnx5j5t6GTv9LWNp1eO7/ydrbVsp1+tu2BmPDiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASbT3PvnndNF02p/p2cJv+4oTK8RUXfqu4nbmT939PfXXabz9wXuX4oX9b/mvy/Y/W1U5C9ZxH73Yc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNF1d4SZ9TfVP/B/4I8+WFzH4ZNeLNbUNQlGHabceUDluO+/v02dYG/WPf/FA2ipYthtL7O93fb6Ectm2L7b9obG40GtbRNAs8ZzZL9e0qJ3LFsiaWVEzJO0svEaQBcrhj0iVkna8Y7FZ0ha3ni+XNKZNfcFoGYT/YKuPyK2NJ5vldQ/VqHtQUmDkjRV+01wcwCa1fQXdBERkqJifGlEDETEwGRNaXZzACZoomHfZnumJDUet9fXEoBWmGjYV0ha3Hi+WNLt9bQDoFWKn9lt3yjpZEmH2N4k6XJJ35R0s+0vSHpO0tmtbHKkf/rw7GLNniddrLnggJfqaKcWUW4XaFox7BFx7hhDp9bcC4AW4go6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii62aqyWjNX11TOX7cfl8qrmPmldUz/AAc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgib3yPPsNR80q1mxZV57q/s8P3lBHO0BX4MgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiThiGjbxg7wjPiET23b9pp19CPVE/l8Z+bqNnVSNv/q8l1jZn+Du8bs7R6MlXoldni0seKR3fYy29ttrx+x7Arbm22vbfw5rc6GAdRvPG/jr5e0aJTlV0XEgsafO+ptC0DdimGPiFWSdrShFwAt1MwXdBfbfqzxNn/MqVptD9pebXv1bu1sYnMAmjHRsF8jaa6kBZK2SPr2WIURsTQiBiJiYLKmTHBzAJo1obBHxLaI2BMRb0q6VtLCetsCULcJhd32zBEvz5K0fqxaAN2heEcY2zdKOlnSIbY3Sbpc0sm2F0gKSc9K+mILewRQAy6qqdDX//7K8cNWvF5cx7Wzf15XO5U2Dr1WrFm09KvFmtlf58KbXtbURTUA9g6EHUiCsANJEHYgCcIOJEHYgSQIO5BE8aKazPZs2145fu9PTyiu40d/+Gix5sxp5XPkJR+YtH+x5qATtxZrhn73Y5Xjk/7tkXH3hO7CkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJMXtFik2bPKtYMXV89fufRP6mnmXGYe9OFleMf+tMH2tQJJoLJKwAQdiALwg4kQdiBJAg7kARhB5Ig7EAShB1IgplqWmzo+U3FmufvqZ7x5oZZBxfXcd70/xx3T1Vmzq+enWfPKR8trqPvnjW19IJ6cWQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYvKIHbPyXY4s1T3z6n9vQiTRnxWCx5sgLH2pDJxgNk1cAKIfd9mzb99j+he3HbV/SWD7D9t22NzQeD2p9uwAmajxH9iFJX46I+ZI+Keki2/MlLZG0MiLmSVrZeA2gSxXDHhFbImJN4/mrkp6QdJikMyQtb5Qtl3Rmq5oE0Lz39EMY24dL+oikByX1R8SWxtBWSf1j/DODkgYlaar2m2ifAJo07i/obO8v6RZJl0bEKyPHYvgr/VG/1o+IpRExEBEDkzWlqWYBTNy4wm57soaDfkNE3NpYvM32zMb4TEnVv40E0FHj+Tbekq6T9EREXDliaIWkxY3niyXdXn97AOoyns/sn5Z0vqR1ttc2ll0m6ZuSbrb9BUnPSTq7NS3Cj04v1tx0XPnM5znTX66jHfSoYtgj4t8ljXpFjiQuhwN6BFfQAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAS3BGmB8z++n3FmsuOPKtYc87vLaujHfQojuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Lg9k+J/Mc/LCzWPHP60qa3c8S/fr5YM++CNU1vB+/G7Z8AEHYgC8IOJEHYgSQIO5AEYQeSIOxAEkxekchR33ulWHP0wedXju87ZVdxHR++8o1izZvFCtSNIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTaOnmF7RclPTdi0SGSXmpbA83rpX57qVept/rt5l4/GBG/OdpAW8P+ro3bqyNioGMNvEe91G8v9Sr1Vr+91OtIvI0HkiDsQBKdDnvzsxu2Vy/120u9Sr3Vby/1+n86+pkdQPt0+sgOoE0IO5BEx8Jue5HtX9p+yvaSTvUxHraftb3O9lrbqzvdzzvZXmZ7u+31I5bNsH237Q2Nx4M62eNIY/R7he3NjX281vZpnezxLbZn277H9i9sP277ksbyrt2/Y+lI2G33Sbpa0mclzZd0ru35nejlPTglIhZ06fnV6yUteseyJZJWRsQ8SSsbr7vF9Xp3v5J0VWMfL4iIO9rc01iGJH05IuZL+qSkixr/rXbz/h1Vp47sCyU9FRFPR8QuSTdJOqNDvfS8iFglacc7Fp8haXnj+XJJZ7a1qQpj9NuVImJLRKxpPH9V0hOSDlMX79+xdCrsh0l6fsTrTY1l3Sok3WX7EduDnW5mnPojYkvj+VZJ/Z1sZpwutv1Y421+170ttn24pI9IelA9uH/5gm58ToyIj2r4Y8dFtk/qdEPvRQyfX+32c6zXSJoraYGkLZK+3dl23s72/pJukXRpRLxt5s4e2b8dC/tmSbNHvJ7VWNaVImJz43G7pNs0/DGk222zPVOSGo/bO9xPpYjYFhF7IuJNSdeqi/ax7ckaDvoNEXFrY3FP7V+pc2F/WNI823Ns7yPpHEkrOtRLJdvTbE9/67mkz0haX/1PdYUVkhY3ni+WdHsHeyl6KzgNZ6lL9rFtS7pO0hMRceWIoZ7av1IHr6BrnFr5jqQ+Scsi4hsdaaTA9hEaPppLw/Ps/7DberV9o6STNfzTy22SLpf0I0k3S/qAhn9WfHZEdMWXYmP0e7KG38KHpGclfXHEZ+KOsX2ipHslrdP/T3d/mYY/t3fl/h0Ll8sCSfAFHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4k8b9IlVrW7m7xTgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number = 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANpUlEQVR4nO3dbYwd5XnG8euy65fa4GAHYlnGKYYYxNISQzY2H6zIiDYxVKqJWlFoUS1R1aQBJUhUjcuHklSN1EgJ9AvFdYSFi8AJFRBcySE4WypCkhpMZGEb1/glUOz4JeAqdpxi4927H/a4WpzdmbN7Zs+c4/v/k1ZnzjzPztwa+9JzzsyzM44IATj3Tai7AADtQdiBJAg7kARhB5Ig7EASv9HOnU32lJiq6e3cZe08dUppn1MfquCf4bz+0i5XTT9a2mfXexcUtp/+xaTSbUz65UBpH5343/I+GLX3dEKn4qSHa2tr2Kdquhb7hnbusnYTL/lYaZ/9v/+R1nf0qf8p7fLyovWlfa7fsbyw/d3vzi3dxpyXjpf20cvbyvtg1DZH34htLX2Mt73M9i7be2yvamVbAMbXmMNue6KkhyTdKKlH0m22e6oqDEC1WhnZF0naExH7IuKUpG9JKv4MCKA2rYR9rqS3h7zf31j3AbZX2t5ie8v7OtnC7gC0YtwvvUXEmojojYjeSSo/Mw1gfLQS9gOS5g15f3FjHYAO1ErYX5G0wPZ825Ml3SppQzVlAajamK+zR8Rp23dL+p6kiZLWRsSOyirrABOmF08AOnjHx0u3sfVv/qmqctrihaueLe5wVRMb+avyLld//fOF7XMe+FETO8JotDSpJiI2StpYUS0AxhFz44EkCDuQBGEHkiDsQBKEHUiCsANJtPXv2TvKot8p7fL3336ksP0TU35YVTUt6/nR7aV93vtZ+Y1D9v3RP1dRTqlvf+Hrhe0r3r23dBsz1/24qnJSYGQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEx02qmThjRmH7yaeLn1giSX09zdwwZ2sTfSY30ad1X/jZJwvbt99XfpOMec9vqaSWy05/rrB9762rK9nPlZOnFbb3T61kNxiCkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBLtn1QzYWJh896/Ln7kyK6eh6usptA9B3sL23d+4nRFe3q/sHWSqpkw07/02tI+V1zz35Xsq1Xhuis49zCyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ior2Tas6fpv5PFt91Zdcd7Zk087V3F5T2qW7STGd4+/emlPbZdcXGNlSCOrQUdttvSjouqV/S6YgonnIGoDZVjOzXR8Q7FWwHwDjiOzuQRKthD0nP237V9srhOtheaXuL7S2nTp1ocXcAxqrVj/FLIuKA7Y9I2mT7vyLixaEdImKNpDWSNGPGxdHi/gCMUUsje0QcaLwekfSMpEVVFAWgemMOu+3pts8/syzp05K2V1UYgGq18jF+tqRnbJ/ZzhMR8VzRL1w+/x19//G1LewSWZgvfJUbc9gjYp+k8ucSAegIXHoDkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTa/0QYoAmv3l9+E5PFv/rL0j4XPPbjKso5JzCyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ioq2Tat7YNl3L5i8u7PPcTze3pZYvfXh3aZ8rd08rbL/3lVtKt7F76aPNltQGW+suADViZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBzRvkdvzJjw4bhu0rLCPh9/+VRh+9dmc604g2v/rvzGFBet5sYUZ9scfToWRz1cGyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk2vtEmAjF+8WTZrZeU7yJZVOKb36Bke37yrWlfd74s/InsbSD2zfXKw1GdiCJ0rDbXmv7iO3tQ9bNsr3J9u7G68zxLRNAq5oZ2R+VdPaE9lWS+iJigaS+xnsAHaw07BHxoqSjZ61eLmldY3mdpJsrrgtAxcZ6gm52RBxsLB+SNHukjrZXSlopSVNVfLdWAOOn5RN0Mfg3siOeO42INRHRGxG9kzSl1d0BGKOxhv2w7TmS1Hg9Ul1JAMbDWMO+QdKKxvIKSc9WUw6A8VL6nd32eklLJV1oe7+k+yX9g6Qnbf+5pLcklT8apSJx8mS7dnXOcX/dFTQvhr3XClpRGvaIuG2EphsqrgXAOGIGHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0d7HP3lWLDaX5zvZngevK2zf+8er21RJucVfKn9E1AWP5XpEFI9/AkDYgSwIO5AEYQeSIOxAEoQdSIKwA0m094kw6Hw8ieWcxcgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJJtWgaw0087/XTTxapo03cKkTIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYVIMPuGL1zwvbr770ttJtvLZofVXlFHrlqw+X9lly7M7SPtOf2lxFOR2vdGS3vdb2Edvbh6z7su0Dtrc2fm4a3zIBtKqZj/GPSlo2zPoHI2Jh42djtWUBqFpp2CPiRUlH21ALgHHUygm6u22/1viYP3OkTrZX2t5ie8v7OtnC7gC0Yqxhf1jSZZIWSjoo6RsjdYyINRHRGxG9kzRljLsD0KoxhT0iDkdEf0QMSPqmpEXVlgWgamMKu+05Q95+VtL2kfoC6Ayl19ltr5e0VNKFtvdLul/SUtsLNfj8kDcllV/MBFCr0rBHxHCzKB4Zh1rQAfrf2FvYfvwQ39i6FdNlgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJbl6BUbn8cy+X9rl02h2lffb97toqyil1eHH5eHb5D2cXtp8+dLiqcmrFyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAkm1eCctvv28qfGXP/vf1HYPvk5JtUA6CKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeScES0bWczPCsW+4a27Q/1mLjg0tI+v3pooLD9P377O1WVU+pfjl1Y2P7En36mdBvx6o6qymnJ5ujTsTjq4doY2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCa6zoxYn/nBxYfsNf/tS6Ta+clFnXNuWpBtv+pPSPgNbXx/3OrjODqA87Lbn2X7B9uu2d9j+YmP9LNubbO9uvM4c/3IBjFUzI/tpSfdGRI+k6yTdZbtH0ipJfRGxQFJf4z2ADlUa9og4GBE/aSwfl7RT0lxJyyWta3RbJ+nm8SoSQOtGdXdZ25dIukbSZkmzI+Jgo+mQpGGfe2t7paSVkjRV08ZaJ4AWNX2CzvZ5kp6SdE9EHBvaFoOn9Ic9rR8RayKiNyJ6J2lKS8UCGLumwm57kgaD/nhEPN1Yfdj2nEb7HElHxqdEAFVo5my8JT0iaWdEPDCkaYOkFY3lFZKerb48AFUpnVRje4mkH0jaJunMHQfu0+D39iclfVTSW5JuiYijRdtiUg2qNNA3r7TPpiv/rQ2VSFeu/nxpn0se3FbaZ+D48ZbqKJpUU3qCLiJekjTsL0siuUCXYAYdkARhB5Ig7EAShB1IgrADSRB2IAnCDiTBnWpwTvvFxo+V9vne1Y8Vtn9owm9WVU6pZX9we2F72ZNnNg98nzvVANkRdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Igkk1SG/i5ZcVtse08hul3vmvG0r73Dz9l03XNJJl84sfm/WfJ7+rYwPvMqkGyIywA0kQdiAJwg4kQdiBJAg7kARhB5IY1VNcgXNR/xt7W97Gsf6pTfRq/Tr7cz/dXNi+6DMnRmxjZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kERbb15h++eS3hqy6kJJ77StgNZ1U73dVKvUXfV2cq2/FREXDdfQ1rD/2s7tLRHRW1sBo9RN9XZTrVJ31dtNtQ7Fx3ggCcIOJFF32NfUvP/R6qZ6u6lWqbvq7aZa/1+t39kBtE/dIzuANiHsQBK1hd32Mtu7bO+xvaquOpph+03b22xvtb2l7nrOZnut7SO2tw9ZN8v2Jtu7G68z66xxqBHq/bLtA41jvNX2TXXWeIbtebZfsP267R22v9hY37HHdyS1hN32REkPSbpRUo+k22z31FHLKFwfEQs79Prqo5KWnbVulaS+iFggqa/xvlM8ql+vV5IebBzjhRGxsc01jeS0pHsjokfSdZLuavxf7eTjO6y6RvZFkvZExL6IOCXpW5KW11RL14uIFyUdPWv1cknrGsvrJN3c1qIKjFBvR4qIgxHxk8bycUk7Jc1VBx/fkdQV9rmS3h7yfn9jXacKSc/bftX2yrqLadLsiDjYWD4kaXadxTTpbtuvNT7md9zHYtuXSLpG0mZ14fHlBF1zlkTEtRr82nGX7U/VXdBoxOD11U6/xvqwpMskLZR0UNI36i3ng2yfJ+kpSfdExLGhbV1yfGsL+wFJ84a8v7ixriNFxIHG6xFJz2jwa0inO2x7jiQ1Xo/UXE+hiDgcEf0RMSDpm+qgY2x7kgaD/nhEPN1Y3VXHV6ov7K9IWmB7vu3Jkm6VVP7M2xrYnm77/DPLkj4taXvxb3WEDZJWNJZXSHq2xlpKnQlOw2fVIcfYtiU9ImlnRDwwpKmrjq9U4wy6xqWVf5Q0UdLaiPhqLYWUsH2pBkdzafA++090Wq2210taqsE/vTws6X5J35H0pKSPavDPim+JiI44KTZCvUs1+BE+JL0p6c4h34lrY3uJpB9I2iZpoLH6Pg1+b+/I4zsSpssCSXCCDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+D/lIVwTCkkbGAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the distribution of classes among train and test dataset, using 'Counter' and plot a bar chart for that:"
      ],
      "metadata": {
        "id": "fKATEQ3QCdab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "plt.bar(*zip(*Counter(y_train).items()), label='Train')\n",
        "plt.bar(*zip(*Counter(y_test).items()), label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "E-VjrRgy0XXN",
        "outputId": "bbc72ac6-7d1b-49ef-fdbd-4604b01040be"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we have nearly equal number of rows for each class, both for train and test datasets.\n",
        "\n",
        "Finally, we'll scale the datasets. Scaling is used to set the pixels values in the range of 0 to 1. Neural networks process inputs using small weight values, and inputs with large integer values e.g. 255 can disrupt or slow down the learning process.\n",
        "Note that if we use imread, the pixels values stored in the arrays are already scaled between 0 and 1. This is because the imread method automatically scales the image. We'll flatten the 2d array to 1d array, in order to feed it to the network as input:"
      ],
      "metadata": {
        "id": "Y-NnS9d1CpwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [img.flatten() / 255 for img in X_train]\n",
        "X_test = [img.flatten() / 255 for img in X_test]"
      ],
      "metadata": {
        "id": "jOrxKFyP2Tjp"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex4MKm79dyH8"
      },
      "source": [
        "## Phase 2: Completing the code\n",
        "Now, we'll complete the code for each component of the network like Dataloader, activation functions, loss function, layer and the entire network. Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "T8P7BPtADw6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "VhfyM5xddyH-"
      },
      "outputs": [],
      "source": [
        "class Dataloader:\n",
        "    '''\n",
        "    This class prepares the dataset for the neural network.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, data, labels, n_classes, batch_size=None, shuffle=False):\n",
        "        '''\n",
        "        This is the constructor. It gets dataset information and initializes the \n",
        "        Dataloader class fields.\n",
        "            Parameters:\n",
        "                data: features your dataset in pandas.Dataframe format.\n",
        "                labels: labels of your dataset in pandas.Dataframe format.\n",
        "                n_classes: number of classes you have in your dataset.\n",
        "                batch_size: the number of samples that will be propagated through the network.\n",
        "                shuffle: boolean value indicating whether or not the dataset should be shuffled\n",
        "        '''\n",
        "\n",
        "        assert len(data)==len(labels)\n",
        "\n",
        "        self.__n_classes = n_classes\n",
        "        self.__batch_size = batch_size\n",
        "        self.__shuffle = shuffle\n",
        "        self.__data = data\n",
        "        self.__onehot_labels = self.__onehot(labels, self.__n_classes)\n",
        "    \n",
        "    def __onehot(self, labels, n_classes):\n",
        "        '''\n",
        "        This private method gets labels and provides one_hot vectors of labels.\n",
        "        For categorical variables where no such ordinal relationship exists,\n",
        "        the integer encoding is not enough.\n",
        "        In this case, a one-hot encoding can be applied to the integer representation.\n",
        "        This is where the integer encoded variable is removed, and a new binary variable is\n",
        "        added for each unique integer value.\n",
        "        example:\n",
        "            red,    green,    blue\n",
        "            1,      0,        0\n",
        "            0,      1,        0\n",
        "            0,      0,        1\n",
        "                Parameters:\n",
        "                        label: lables of your dataset in pandas.Dataframe format.\n",
        "                        n_classes: number of classes you have in your dataset.\n",
        "                \n",
        "                Returns:\n",
        "                    onehot_vectors: onehot vectors of the labels\n",
        "        '''\n",
        "        onehot_vectors = pd.DataFrame(OneHotEncoder().fit_transform(labels).toarray())\n",
        "        return onehot_vectors\n",
        "    \n",
        "    def __shuffle_dataset(self):\n",
        "        '''\n",
        "        This private method shuffles your dataset.\n",
        "        It uses data and onehot_labels to shuffle them\n",
        "        symmetrical.\n",
        "        '''\n",
        "        perm = np.random.permutation((self.__data.shape[0]))\n",
        "        self.__data = self.__data[perm]\n",
        "        self.__onehot_labels = self.__onehot_labels[perm]\n",
        "\n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        The __iter__() function returns an iterator for the\n",
        "        given object (array, set, tuple, etc., or custom objects).\n",
        "        This will return your dataset in the batch_size given. This should\n",
        "        be used to provide data for the neural network.\n",
        "        '''\n",
        "        \n",
        "        if self.__shuffle:\n",
        "            self.__shuffle_dataset()\n",
        "            \n",
        "        if self.__batch_size==None:\n",
        "            yield (np.matrix(self.__data), np.matrix(self.__onehot_labels))\n",
        "            return\n",
        "\n",
        "        for idx in range(0, len(self.__data), self.__batch_size):\n",
        "            yield (np.matrix(self.__data[idx:idx+self.__batch_size]), \n",
        "                   np.matrix(self.__onehot_labels[idx:idx+self.__batch_size]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HURyCdkbdyH_"
      },
      "source": [
        "### Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "b7tt4CnqdyH_"
      },
      "outputs": [],
      "source": [
        "class Identical:\n",
        "    '''\n",
        "    This is the Identical activation function. This activation function just\n",
        "    return the value it gets.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self):\n",
        "        '''\n",
        "        This is the constructor. It does not have any fields\n",
        "        as a result, there is no need to do anything in the constructor.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    \n",
        "    def __val(self, matrix):\n",
        "        '''\n",
        "        This private method gets a matrix and uses the activity function on that.\n",
        "        As this is an identical activity function, it just \n",
        "        returns np.matrix of the input.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                identical_value: np.matrix of input with float datatypes\n",
        "        '''\n",
        "        identical_value = np.matrix(matrix, dtype=float)\n",
        "        return identical_value\n",
        "\n",
        "    def derivative(self, matrix):\n",
        "        '''\n",
        "        This method returns the derivation of the input.\n",
        "        As the derivation of x is one, this method returns\n",
        "        a matrix of one with the shape of the input matrix.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                identical_derivative: np.matrix of ones with matrix shape\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        identical_derivative = np.matrix(np.full(np.shape(temp), 1.))\n",
        "        return identical_derivative\n",
        "    \n",
        "    def __call__(self, matrix):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _value method output.\n",
        "            \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                __val(matrix): __val return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(matrix)\n",
        "    \n",
        "\n",
        "class Relu:\n",
        "    '''\n",
        "    This is the Relu activation function. \n",
        "    The rectified linear activation function or ReLU for short\n",
        "    is a piecewise linear function that will output the input directly\n",
        "    if it is positive, otherwise, it will output zero.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self): \n",
        "        '''\n",
        "        This is the constructor. It does not have any fields\n",
        "        as a result, there is no need to do anything in the constructor.\n",
        "        '''\n",
        "        pass\n",
        "        \n",
        "    def __val(self, matrix):\n",
        "        '''\n",
        "        This private method gets a matrix and uses the activity function on that.\n",
        "        It will set 0 in the matrix if the value is less than 0 else, it returns the value itself.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                relu_value: np.matrix of relu activation function result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        relu_value = np.maximum(0, temp)\n",
        "        return relu_value\n",
        "\n",
        "    def derivative(self, matrix):\n",
        "        '''\n",
        "        Returns the derivation value of relu function on input matrix.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                relu_derivative: np.matrix of relu activation function derivation result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        relu_derivative = np.matrix(np.full(np.shape(temp), np.where(temp >= 0, 1, 0)))\n",
        "        return relu_derivative\n",
        "    \n",
        "    def __call__(self, matrix):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _relu method output.\n",
        "            \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                __relu(matrix): __relu return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(matrix)\n",
        "\n",
        "    \n",
        "class LeakyRelu:\n",
        "    '''\n",
        "    This is the Leaky Relu activation function. \n",
        "    Leaky Rectified Linear Unit, or Leaky ReLU,\n",
        "    is a type of activation function based on a ReLU,\n",
        "    but it has a small slope for negative values instead\n",
        "    of a flat slope.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, negative_slope=0.01):\n",
        "        '''\n",
        "        This is the constructor.\n",
        "        It sets negative_slope field.\n",
        "            Parameters:\n",
        "                negative_slope: slope for negative input values\n",
        "        '''\n",
        "        self.negative_slope = negative_slope\n",
        "    \n",
        "    def __val(self, matrix):\n",
        "        '''\n",
        "        This private method gets a matrix and uses the activity function on that.\n",
        "        It will set negative_slope*value in the matrix if the value is less than 0, else it\n",
        "        returns the value itself.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                relu_value: np.matrix of relu activation function result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        leaky_relu_value = np.where(temp < 0, temp * self.negative_slope, temp)\n",
        "        return leaky_relu_value\n",
        "\n",
        "    def derivative(self, matrix):\n",
        "        '''\n",
        "        Returns the derivation value of leaky relu function on input matrix.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                leacky_relu_derivative: np.matrix of leaky relu activation function derivation result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        leaky_relu_derivative = np.matrix(np.full(np.shape(temp), np.where(temp >= 0, 1, self.negative_slope)))\n",
        "        return leaky_relu_derivative\n",
        "    \n",
        "    def __call__(self, matrix):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _val method output.\n",
        "            \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                __val(matrix): __val return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(matrix)\n",
        "\n",
        "    \n",
        "class Sigmoid:\n",
        "    '''\n",
        "    A sigmoid function is a mathematical function having a\n",
        "    characteristic \"S\"-shaped curve or sigmoid curve.\n",
        "    It return S(x)=1/(1+e^-x)\n",
        "    '''\n",
        "    \n",
        "    def __init__(self): \n",
        "        '''\n",
        "        This is the constructor. It does not have any fields\n",
        "        as a result, there is no need to do anything in the constructor.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def __val(self, matrix):\n",
        "        '''\n",
        "        Returns 1/(1+e^-x) of values\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                sigmoid_value: np.matrix of sigmoid activation function result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        sigmoid_value = 1/(1 + np.exp(-temp))\n",
        "        return sigmoid_value\n",
        "\n",
        "    def derivative(self, matrix):\n",
        "        '''\n",
        "        Returns the derivation value of sigmoid function on input matrix.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                sigmoid_derivative: np.matrix of sigmoid activation function derivation result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        temp = self.__val(temp)\n",
        "        sigmoid_derivative = np.multiply(temp, (1 - temp))\n",
        "        return sigmoid_derivative\n",
        "    \n",
        "    def __call__(self, matrix):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _val method output.\n",
        "            \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                __val(matrix): __val return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(matrix)\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    '''\n",
        "    The softmax function, also known as softargmax or normalized\n",
        "    exponential function is a generalization of the logistic\n",
        "    function to multiple dimensions. It is used in multinomial logistic\n",
        "    regression and is often used as the last activation function of a neural\n",
        "    network to normalize the output of a network to a probability distribution\n",
        "    over predicted output classes, based on Luce's choice axiom.\n",
        "    Softmax return (e^x_i / (e^x_j for j = 1, ..., J))\n",
        "    '''\n",
        "        \n",
        "    def __init__(self): \n",
        "        '''\n",
        "        This is the constructor. It does not have any fields\n",
        "        as a result, there is no need to do anything in the constructor.\n",
        "        '''\n",
        "        pass\n",
        "        \n",
        "\n",
        "    def __val(self, matrix):\n",
        "        '''\n",
        "        This private method gets a matrix and uses the softmax on that.\n",
        "        Softmax return (e^x_i / (e^x_j for j = 1, ..., J))\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                softmax_value: np.matrix of softmax activation function result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)    \n",
        "        softmax_value = np.apply_along_axis(lambda row: np.exp(row - np.max(row))/np.sum(np.exp(row - np.max(row))), 1, temp)\n",
        "        return softmax_value\n",
        "    \n",
        "    def __call__(self, matrix):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _val method output.\n",
        "            \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                __val(matrix): __val return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(matrix)\n",
        "    \n",
        "class Tanh:\n",
        "    '''\n",
        "    tanh is also like logistic sigmoid but better. \n",
        "    The range of the tanh function is from (-1 to 1).\n",
        "    tanh is also sigmoidal (s - shaped).\n",
        "    '''\n",
        "    \n",
        "    def __init__(self): \n",
        "        '''\n",
        "        This is the constructor. It does not have any fields\n",
        "        as a result, there is no need to do anything in the constructor.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def __val(self, matrix): # check\n",
        "        '''\n",
        "        This private method gets a matrix and uses the activity function on that.\n",
        "        It performs Tanh on the values.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                tanh_value: np.matrix of Tanh activation function result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        tanh_value = np.tanh(temp)\n",
        "        tanh_value = np.matrix(tanh_value)\n",
        "        return tanh_value\n",
        "\n",
        "    def derivative(self, matrix):\n",
        "        '''\n",
        "        Returns the derivation value of Tanh function on input matrix.\n",
        "        \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                sigmoid_derivative: np.matrix of Tanh activation function derivation result\n",
        "        '''\n",
        "        temp = np.matrix(matrix, dtype=float)\n",
        "        tanh_derivative = 1 - np.power(self.__val(temp), 2)\n",
        "        return tanh_derivative\n",
        "    \n",
        "    def __call__(self, matrix):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _val method output.\n",
        "            \n",
        "            Parameters:\n",
        "                matrix: np.matrix of values\n",
        "            Returns:\n",
        "                __val(matrix): __val return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbAFGwNQdyIB"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "DsqP05GWdyIB"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy: #(with softmax)\n",
        "    '''\n",
        "    Cross-entropy is a measure of the difference between two probability\n",
        "    distributions for a given random variable or set of events. You might\n",
        "    recall that information quantifies the number of bits required to encode\n",
        "    and transmit an event.\n",
        "    The above image can help you.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self): \n",
        "        '''\n",
        "        This is the constructor. It does not have any fields\n",
        "        as a result, there is no need to do anything in the constructor.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def __val(self, true_val, expected_val):\n",
        "        '''\n",
        "        L(y^, y) = -  (y^(k)log (y^)^(k)) for k in K\n",
        "        Parameters:\n",
        "            true_val: calculated values (generated by neural network)\n",
        "            expected_val: real values in dataset\n",
        "        Returns:\n",
        "            cross_entropy_value: cross entropy of inputs\n",
        "        '''\n",
        "        assert np.shape(true_val)==np.shape(expected_val)\n",
        "        \n",
        "        temp = np.matrix(true_val, dtype=float)\n",
        "        output = np.log(Softmax()(temp))\n",
        "        expected = np.matrix(expected_val, dtype=float)\n",
        "        \n",
        "        cross_entropy_value =  -1 * np.sum(np.multiply(expected, output), axis=1)\n",
        "        \n",
        "        return cross_entropy_value\n",
        "    \n",
        "        \n",
        "    def derivative(self, true_val, expected_val):\n",
        "        '''\n",
        "        Returns derivation of cross entropy.\n",
        "            Parameters:\n",
        "                true_val: calculated values (generated by neural network)\n",
        "                expected_val: real values in dataset\n",
        "            Returns:\n",
        "                cross_entropy_derivative: cross entropy derivation of inputs\n",
        "        '''\n",
        "        assert np.shape(true_val)==np.shape(expected_val)\n",
        "                                           \n",
        "        temp = np.matrix(true_val, dtype=float)\n",
        "        output = Softmax()(temp)\n",
        "        expected = np.matrix(expected_val, dtype=float)\n",
        "                                           \n",
        "        cross_entropy_derivative = output - expected\n",
        "                                           \n",
        "        return cross_entropy_derivative\n",
        "    \n",
        "    def __call__(self, true_val, expected_val):\n",
        "        '''\n",
        "        __call__ is a special function in Python that, when implemented inside a class,\n",
        "        gives its instances (objects) the ability to behave like a function.\n",
        "        Here we return the _val method output.\n",
        "            \n",
        "            Parameters:\n",
        "                true_val: calculated values (generated by neural network)\n",
        "                expected_val: real values in dataset\n",
        "            Returns:\n",
        "                __val(matrix): __val return value for the input matrix\n",
        "        '''\n",
        "        return self.__val(true_val, expected_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmw-yO4VdyIC"
      },
      "source": [
        "### Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "uNLF4j7JdyIC"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    '''\n",
        "    The layer class is used to define neural network layers.\n",
        "    It stores all needed information for each layer, such as neurons count,\n",
        "    weight matrix, bias, the output after applying the activation function, etc.\n",
        "    '''\n",
        "\n",
        "    DEFAULT_LOW, DEFAULT_HIGH, DEFAULT_MEAN, DEFAULT_VAR = 0, 0.05, 0., 1.\n",
        "  \n",
        "    def __init__(self, input_size, output_size, activation=Identical(), initial_weight='uniform', \n",
        "                 **initializing_parameters):\n",
        "        '''\n",
        "        Parameters:\n",
        "            input_size: the size of the input of this layer.\n",
        "            output_size: the size of the output after this layer.\n",
        "            activation: the activation function. It can be initialized to either of the previously defined activation classes.\n",
        "                               default is an Identical activation function.\n",
        "            initial_weight: either normal or uniform. It defines the method for weight initialization.\n",
        "        '''\n",
        "        \n",
        "        assert type(initial_weight)==str, 'Undefined activation function!'\n",
        "        \n",
        "        self.__weight_initializer_dict = {'uniform':self.__uniform_weight, \n",
        "                                          'normal':self.__normal_weight, \n",
        "                                          'zero':self.__zero_weight}\n",
        "        \n",
        "        assert initial_weight in self.__weight_initializer_dict, 'Undefined weight initialization function!'\n",
        "\n",
        "\n",
        "        self.__n_neurons = output_size\n",
        "        weight_initializer = self.__weight_initializer_dict[initial_weight]\n",
        "        self.__weight = weight_initializer(input_size, self.__n_neurons, **initializing_parameters)\n",
        "        self.__bias = weight_initializer(1, self.__n_neurons, **initializing_parameters)\n",
        "        self.__activation = activation\n",
        "        \n",
        "        self.__last_input = None\n",
        "        self.__last_activation_input = None\n",
        "        self.__last_activation_output = None\n",
        "        self.__last_activation_derivative = None\n",
        "        \n",
        "    def forward(self, layer_input):\n",
        "        '''\n",
        "        It calculates the output of this layer for the layer_input argument.\n",
        "        This method also stores __last_input, __last_activation_input, and __last_activation_derivative\n",
        "        for future use in backpropagation.\n",
        "        Parameters:\n",
        "            layer_input: 2d np.matrix representing the input matrix of this layer.\n",
        "        Returns:\n",
        "            Final output of this layer after applying the activation function.\n",
        "        '''\n",
        "        assert np.ndim(layer_input)==2\n",
        "        assert np.size(self.__weight,0) == np.size(layer_input,1)\n",
        "        \n",
        "        self.__last_input = np.matrix(layer_input, dtype=float)\n",
        "        self.__last_activation_input = np.add(np.dot(self.__last_input, self.__weight), self.__bias)\n",
        "\n",
        "        self.__last_activation_output = self.__activation(self.__last_activation_input)\n",
        "\n",
        "        self.__last_activation_derivative = self.__activation.derivative(self.__last_activation_input)        \n",
        "        return self.__last_activation_output\n",
        "    \n",
        "    def update_weights(self, backprop_tensor, lr):\n",
        "        '''\n",
        "        It updates Layer weights according to the backpropagation matrix and learning rate.\n",
        "        This method updates bias values as well.\n",
        "        Parameters:\n",
        "            backprop_tensor: 2d np.matrix passed from the next layer containing gradient values.\n",
        "            lr: learning rate\n",
        "        Returns:\n",
        "            backprop_tensor to be used by the previous layer.\n",
        "        '''\n",
        "        assert np.ndim(backprop_tensor)==2\n",
        "        assert np.size(backprop_tensor,0) == np.size(self.__last_activation_derivative,0)\n",
        "        assert np.size(backprop_tensor,1) == self.__n_neurons\n",
        "        \n",
        "        ones = np.matrix(np.ones((np.size(backprop_tensor,axis=0), 1)))\n",
        "\n",
        "        dy = np.multiply(backprop_tensor, self.__last_activation_derivative)\n",
        "        db = np.matmul(ones.T, dy)\n",
        "        dw = np.matmul(self.__last_input.T, dy)\n",
        "        \n",
        "        backprop_tensor = np.matmul(dy, np.transpose(self.__weight))\n",
        "\n",
        "        self.__weight -= lr * dw\n",
        "        self.__bias -= lr * db\n",
        "        \n",
        "        return backprop_tensor\n",
        "\n",
        "    def __uniform_weight(self, dim1, dim2, **initializing_parameters):\n",
        "        '''\n",
        "        Initializes weights as a uniform distribution between low and high values.\n",
        "        It uses default low and high values unless low or high are passed in initializing_parameters.\n",
        "        Parameters:\n",
        "            dim1: the size of the first dimension of weights.\n",
        "            dim2: the size of the second dimension of weights.\n",
        "            initializing_parameters: other initializing parameters; it can include custom low or high values.\n",
        "        Returns:\n",
        "            np.matrix with size (dim1, dim2) initialized using uniformly distributed values.\n",
        "        '''\n",
        "        low, high = self.DEFAULT_LOW, self.DEFAULT_HIGH\n",
        "        if 'low' in initializing_parameters.keys(): low = initializing_parameters['low']\n",
        "        if 'high' in initializing_parameters.keys(): high = initializing_parameters['high']\n",
        "        \n",
        "        weights = np.matrix(np.random.uniform(low, high, size=(dim1, dim2)))\n",
        "        return weights\n",
        "\n",
        "    def __normal_weight(self, dim1, dim2, **initializing_parameters):\n",
        "        '''\n",
        "        Initializes weights as a normal distribution with mean and var values.\n",
        "        It uses default mean and variance values unless mean or var are passed in initializing_parameters.\n",
        "        Parameters:\n",
        "            dim1: the size of the first dimension of weights.\n",
        "            dim2: the size of the second dimension of weights.\n",
        "            initializing_parameters: other initializing parameters; it can include custom mean or var values.\n",
        "        Returns:\n",
        "            np.matrix with size (dim1, dim2) initialized using normaly distributed values.\n",
        "        ''' \n",
        "        mean, var = self.DEFAULT_MEAN, self.DEFAULT_VAR\n",
        "        if 'mean' in initializing_parameters.keys(): mean = initializing_parameters['mean']\n",
        "        if 'var' in initializing_parameters.keys(): var = initializing_parameters['var']\n",
        "        weights = np.matrix(np.random.normal(mean, np.sqrt(var), size=(dim1, dim2)))\n",
        "        return weights\n",
        "    \n",
        "    # Not necessary\n",
        "    def __zero_weight(self, dim1, dim2, **initializing_parameters):\n",
        "        '''\n",
        "        Initializes weights as 0.\n",
        "        Parameters:\n",
        "            dim1: the size of the first dimension of weights.\n",
        "            dim2: the size of the second dimension of weights.\n",
        "        Returns:\n",
        "            np.matrix with size (dim1, dim2) initialized using 0 values.\n",
        "        ''' \n",
        "        weights = np.matrix(np.zeros(shape=(dim1,dim2)))\n",
        "        return weights\n",
        "    \n",
        "    @property\n",
        "    def n_neurons(self): return self.__n_neurons\n",
        "    \n",
        "    @property\n",
        "    def weight(self): return self.__weight\n",
        "    \n",
        "    @property\n",
        "    def bias(self): return self.__bias\n",
        "    \n",
        "    @property\n",
        "    def activation(self): return self.__activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB7Hu5kCdyIC"
      },
      "source": [
        "### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Yd6DchbMdyID"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN:\n",
        "    '''\n",
        "    This class is used in order to implement custom feed-forward neural networks.\n",
        "    The FeedForwardNN class stores a list of layers that determines all network layers.\n",
        "    It also consists of the learning rate and loss function.\n",
        "    '''\n",
        "    def __init__(self, input_shape):\n",
        "        '''\n",
        "        Parameters:\n",
        "            input_shape: the size of the first input to our neural network.\n",
        "        '''\n",
        "        \n",
        "        self.__input_shape = input_shape\n",
        "        self.__output_shape = None\n",
        "        \n",
        "        self.__layers_list = []\n",
        "        \n",
        "        self.__lr = None\n",
        "        self.__loss = None\n",
        "\n",
        "        \n",
        "    def add_layer(self, n_neurons, activation=Relu(), initial_weight='uniform', **initializing_parameters):\n",
        "        '''\n",
        "         This method adds a new custom layer to the layers_list.\n",
        "         Parameters:\n",
        "             n_neurons: number of neurons in this layer\n",
        "             activation: the activation function of this layer, default is Relu\n",
        "             initial_weight: either a uniform or normal, default is uniform\n",
        "             initializing_parameters: other initializing parameters such as low, high, mean, var, etc\n",
        "        '''\n",
        "         \n",
        "        assert type(n_neurons)==int, \"Invalid number of neurons for the layer!\"\n",
        "        assert n_neurons>0, \"Invalid number of neurons for the layer!\"\n",
        "        \n",
        "        n_prev_neurons = self.__input_shape if len(self.__layers_list)==0 else self.__layers_list[-1].n_neurons\n",
        "        new_layer = Layer(n_prev_neurons, n_neurons, activation, initial_weight, **initializing_parameters)\n",
        "        self.__layers_list.append(new_layer)\n",
        "        self.__output_shape = self.__layers_list[-1].n_neurons \n",
        "      \n",
        "    \n",
        "    def set_training_param(self, loss=CrossEntropy(), lr=1e-3):\n",
        "        '''\n",
        "        This method is used to set training parameters.\n",
        "        Parameters:\n",
        "            loss: loss function, default is CrossEntropy\n",
        "            lr: learning rate, default is 1e-3\n",
        "        '''\n",
        "        assert self.__layers_list, \"Uncomplete model!\"\n",
        "        self.__loss = loss\n",
        "        self.__lr = lr\n",
        "    \n",
        "    \n",
        "    def forward(self, network_input):\n",
        "        '''\n",
        "        This method calculates the output of the complete neural network for a passed input.\n",
        "        Parameters:\n",
        "            network_input: input of the neural network\n",
        "        Returns:\n",
        "            network_output: output of the neural network after forwarding the network_input\n",
        "        '''\n",
        "        assert type(self.__output_shape) != None, \"Model is not compiled!\"\n",
        "\n",
        "        network_output = network_input\n",
        "        for layer in self.__layers_list:\n",
        "            network_output = layer.forward(network_output)\n",
        "        \n",
        "        return network_output\n",
        "    \n",
        "    \n",
        "    def fit(self, epochs, trainloader, testloader=None, print_results=True):\n",
        "        '''\n",
        "        This method trains the neural network using specified parameters.\n",
        "        It runs the __train private method epoch times and fills the log dictionary.\n",
        "        Parameters:\n",
        "            epochs: number of epochs to run\n",
        "            trainloader: DataLoader for train data\n",
        "            testloader: DataLoader for test data\n",
        "            print_results: whether or not to print the results\n",
        "        Returns:\n",
        "            log: complete log of the training process as a dictionary consisting of\n",
        "            train_accuracy, train_loss, test_accuracy, test_loss\n",
        "        '''\n",
        "        \n",
        "        assert type(self.__output_shape) != None, \"Model is not compiled!\"\n",
        "        assert type(self.__lr) != None and type(self.__loss) != None, \"Training paramenters are not set!\"\n",
        "\n",
        "        log = {\"train_accuracy\":[], \"train_loss\":[], \"test_accuracy\":[], \"test_loss\":[]}\n",
        "        \n",
        "        for epoch in range(1, epochs+1):\n",
        "            \n",
        "            if print_results: \n",
        "                print('Epoch {}:'.format(epoch)) \n",
        "                \n",
        "            average_accuracy, average_loss = self.__train(trainloader)\n",
        "            log['train_accuracy'].append(average_accuracy)\n",
        "            log['train_loss'].append(average_loss)\n",
        "            if print_results:\n",
        "                print('\\tTrain: Average Accuracy: {}\\tAverage Loss: {}'.format(average_accuracy, average_loss))\n",
        "            \n",
        "            if type(testloader) != type(None):\n",
        "                average_accuracy, average_loss = self.__test(testloader)\n",
        "                log['test_accuracy'].append(average_accuracy)\n",
        "                log['test_loss'].append(average_loss)\n",
        "                if print_results:\n",
        "                    print('\\tTest: Average Accuracy: {}\\tAverage Loss: {}'.format(average_accuracy, average_loss))\n",
        "                    \n",
        "        return log\n",
        "    \n",
        "    \n",
        "    def __train(self, trainloader):\n",
        "        '''\n",
        "        Trains the neural network for one epoch.\n",
        "        Parameters:\n",
        "            trainloader: A DataLoader consisting of train data\n",
        "        Returns:\n",
        "            batch_accuracy, batch_loss: mean of all batch_accuracies, batch_losses\n",
        "        '''\n",
        "        batch_accuracies, batch_losses = [], []\n",
        "        for x_train, y_train in trainloader:\n",
        "            batch_accuracy, batch_loss = self.__train_on_batch(x_train, y_train)\n",
        "            batch_accuracies.append(batch_accuracy)\n",
        "            batch_losses.append(batch_loss)\n",
        "        return np.mean(batch_accuracies), np.mean(batch_losses)\n",
        "    \n",
        "    \n",
        "    def __test(self, testloader):\n",
        "        '''\n",
        "        Test the neural network using a testloader.\n",
        "        Parameters:\n",
        "            testloader: A DataLoader of test data\n",
        "        Returns:\n",
        "            batch_accuracy, batch_loss: mean of all batch_accuracies, batch_losses\n",
        "        '''\n",
        "        batch_accuracies, batch_losses = [], []\n",
        "        for x_test, y_test in testloader:\n",
        "            batch_accuracy, batch_loss = self.__test_on_batch(x_test, y_test)\n",
        "            batch_accuracies.append(batch_accuracy)\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "        return np.mean(batch_accuracies), np.mean(batch_losses)\n",
        "\n",
        "    \n",
        "    def __train_on_batch(self, x_batch, y_batch):\n",
        "        '''\n",
        "        Trains the neural network for one batch of train data.\n",
        "        Parameters:\n",
        "            x_batch: one batch data\n",
        "            y_batch: labels for one batch\n",
        "        Returns:\n",
        "            (batch_accuracy, batch_average_loss)\n",
        "        '''\n",
        "        network_output = self.forward(x_batch)\n",
        "        batch_accuracy = self.__compute_accuracy(network_output, y_batch)\n",
        "        batch_average_loss = np.mean(self.__loss(network_output, y_batch))\n",
        "\n",
        "        self.__update_weights(network_output, y_batch)\n",
        "\n",
        "        return (batch_accuracy, batch_average_loss)\n",
        "        \n",
        "        \n",
        "    def __test_on_batch(self, x_batch, y_batch):\n",
        "        '''\n",
        "        Tests the neural network for one batch of test data.\n",
        "        Parameters:\n",
        "            x_batch: one batch data\n",
        "            y_batch: labels for one batch\n",
        "        Returns:\n",
        "            (batch_accuracy, batch_average_loss)\n",
        "        '''  \n",
        "        network_output = self.forward(x_batch)\n",
        "\n",
        "        batch_accuracy = self.__compute_accuracy(network_output, y_batch)\n",
        "        batch_average_loss = np.mean(self.__loss(network_output, y_batch))\n",
        "        \n",
        "        return (batch_accuracy, batch_average_loss)\n",
        "            \n",
        "        \n",
        "    def __get_labels(self, outputs):\n",
        "        '''\n",
        "        Parameters:\n",
        "            outputs: output of the neural network\n",
        "        Returns:\n",
        "            labels: labels generated from the outputs of the neural network\n",
        "        '''\n",
        "        labels = outputs.argmax(1)\n",
        "        return labels\n",
        "    \n",
        "    \n",
        "    def __compute_accuracy(self, output, expected_output):\n",
        "        '''\n",
        "        Computes accuracy by comparing output and expected_output.\n",
        "        Parameters:\n",
        "            output: actual output of the neural network\n",
        "            expected_output: expected output\n",
        "        Returns:\n",
        "            accuracy\n",
        "        '''\n",
        "        labels = self.__get_labels(output)\n",
        "        expected_labels = self.__get_labels(expected_output)\n",
        "        accuracy = np.count_nonzero(labels == expected_labels) / np.size(output, axis=0) * 100\n",
        "        return accuracy\n",
        "    \n",
        "    \n",
        "    def __update_weights(self, output, y_train):\n",
        "        '''\n",
        "        Updates weights of all layers according to neural network output and labels.\n",
        "        Parameters:\n",
        "            output: output of the neural network\n",
        "            y_train: y labels for one batch of train data\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        backpropagation_tensor = self.__loss.derivative(output, y_train)\n",
        "        for layer in reversed(self.__layers_list):\n",
        "            backpropagation_tensor = layer.update_weights(backpropagation_tensor, self.__lr)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(data=X_train)\n",
        "X_test = pd.DataFrame(data=X_test)"
      ],
      "metadata": {
        "id": "mc6bN4gvDhlH"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.DataFrame(data=y_train)\n",
        "y_test = pd.DataFrame(data=y_test)"
      ],
      "metadata": {
        "id": "55pgG5OtEAXb"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVuP_v9ddyID"
      },
      "source": [
        "## Phase 3: Data classification\n",
        "First we set the default values, according to the description for the network:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = 25 * 25\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "2oumeQtKegdD"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll implement a function to train a neural network with parameters for later uses:"
      ],
      "metadata": {
        "id": "XzgosRBJGEkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(activation=Relu(), initial='uniform', print_results=True, lr=LEARNING_RATE, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "  TRAINLOADER = [batch for batch in Dataloader(X_train, y_train, n_classes=10, batch_size=batch_size, shuffle=False)]\n",
        "  TESTLOADER = [batch for batch in Dataloader(X_test, y_test, n_classes=10, batch_size=batch_size, shuffle=False)]\n",
        "\n",
        "  network = FeedForwardNN(INPUT_SHAPE)\n",
        "  network.add_layer(50, activation=activation, initial_weight=initial)\n",
        "  network.add_layer(10, activation=Identical(), initial_weight=initial)\n",
        "  network.set_training_param(loss=CrossEntropy(), lr=lr)\n",
        "\n",
        "  log = network.fit(epochs, TRAINLOADER, TESTLOADER, print_results=print_results)\n",
        "  return log"
      ],
      "metadata": {
        "id": "O2qxPWiveNem"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the network\n",
        "Let's train a model with the default values and see the results:"
      ],
      "metadata": {
        "id": "NqmxC1BPGOnl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "mGTaoT6_dyID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62cbce0-3263-4fcd-a643-ca3a0be85585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\tTrain: Average Accuracy: 90.19636576787808\tAverage Loss: 0.3921189951967507\n",
            "\tTest: Average Accuracy: 95.97953464673913\tAverage Loss: 0.14661961070297583\n",
            "Epoch 2:\n",
            "\tTrain: Average Accuracy: 96.1398495506057\tAverage Loss: 0.13914547399925825\n",
            "\tTest: Average Accuracy: 96.84379245923913\tAverage Loss: 0.11749605379278023\n",
            "Epoch 3:\n",
            "\tTrain: Average Accuracy: 97.00444509574052\tAverage Loss: 0.10995813373349489\n",
            "\tTest: Average Accuracy: 97.45902683423913\tAverage Loss: 0.09773412693782993\n",
            "Epoch 4:\n",
            "\tTrain: Average Accuracy: 97.56252442360297\tAverage Loss: 0.08952739984227544\n",
            "\tTest: Average Accuracy: 97.76176120923913\tAverage Loss: 0.0844251245290571\n",
            "Epoch 5:\n",
            "\tTrain: Average Accuracy: 97.92643610785463\tAverage Loss: 0.07569115315355837\n",
            "\tTest: Average Accuracy: 97.99125339673913\tAverage Loss: 0.07535506537964856\n",
            "Epoch 6:\n",
            "\tTrain: Average Accuracy: 98.18410511918718\tAverage Loss: 0.06578145992036968\n",
            "\tTest: Average Accuracy: 98.19144870923913\tAverage Loss: 0.06900892950561127\n",
            "Epoch 7:\n",
            "\tTrain: Average Accuracy: 98.41002344665885\tAverage Loss: 0.05839139342463239\n",
            "\tTest: Average Accuracy: 98.26469089673913\tAverage Loss: 0.06432758132767777\n",
            "Epoch 8:\n",
            "\tTrain: Average Accuracy: 98.56267096522079\tAverage Loss: 0.05263666139390531\n",
            "\tTest: Average Accuracy: 98.349609375\tAverage Loss: 0.060814060418796924\n",
            "Epoch 9:\n",
            "\tTrain: Average Accuracy: 98.68967370066431\tAverage Loss: 0.04794613839615447\n",
            "\tTest: Average Accuracy: 98.3642578125\tAverage Loss: 0.058194818186641795\n",
            "Epoch 10:\n",
            "\tTrain: Average Accuracy: 98.8056858147714\tAverage Loss: 0.04401609425515578\n",
            "\tTest: Average Accuracy: 98.369140625\tAverage Loss: 0.05603060261045077\n",
            "Epoch 11:\n",
            "\tTrain: Average Accuracy: 98.88994724501758\tAverage Loss: 0.040694558102582636\n",
            "\tTest: Average Accuracy: 98.3935546875\tAverage Loss: 0.054184825345900356\n",
            "Epoch 12:\n",
            "\tTrain: Average Accuracy: 98.9681027745213\tAverage Loss: 0.03778710614065399\n",
            "\tTest: Average Accuracy: 98.3984375\tAverage Loss: 0.05269548150398037\n",
            "Epoch 13:\n",
            "\tTrain: Average Accuracy: 99.03160414224307\tAverage Loss: 0.03522106781633772\n",
            "\tTest: Average Accuracy: 98.447265625\tAverage Loss: 0.05123293818201809\n",
            "Epoch 14:\n",
            "\tTrain: Average Accuracy: 99.11342321219226\tAverage Loss: 0.03289210409784665\n",
            "\tTest: Average Accuracy: 98.4716796875\tAverage Loss: 0.05018266416773833\n",
            "Epoch 15:\n",
            "\tTrain: Average Accuracy: 99.16959749902306\tAverage Loss: 0.030888187761969797\n",
            "\tTest: Average Accuracy: 98.49609375\tAverage Loss: 0.04913085408990622\n"
          ]
        }
      ],
      "source": [
        "log = train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Weighting\n",
        "What if we initialized the networks weights as 0?\n",
        "Would it make any difference?\n",
        "Let's see:"
      ],
      "metadata": {
        "id": "d6o3KIuDGeCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log = train_model(activation=Relu(), initial='zero')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrU4pWkNe81b",
        "outputId": "236bca7a-9d64-4db8-a6cd-9671eb2b5867"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\tTrain: Average Accuracy: 10.036342321219227\tAverage Loss: 2.3027118010651564\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.3026120915045603\n",
            "Epoch 2:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088282002914\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.3026121673157958\n",
            "Epoch 3:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.302708831302017\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336058\n",
            "Epoch 4:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.30270883130276\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.3026121673360627\n",
            "Epoch 5:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 6:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 7:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 8:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 9:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 10:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 11:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 12:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 13:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 14:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n",
            "Epoch 15:\n",
            "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.3027088313027604\n",
            "\tTest: Average Accuracy: 9.98726222826087\tAverage Loss: 2.302612167336063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When all the weights are set to zero, the derivatives will remain the same. As a result, neurons will learn the same features in each epoch. This problem is known as network failing to break symmetry.\n",
        "\n",
        "As you can see above, the accuracy almost never changes. This is because in each iteration of gradient descent, we will find the gradient for the previous layer by multiplying the existing weight with a gradient obtained by backpropagation from the next layer. If the initial weight is zero, multiplying it by any gradient will set the gradient to be zero. Due to zero gradient, the gradient descent process wont change the weight which means each iteration has no effect on the weights we are trying to optimize.\n",
        "\n",
        "Every neuron in the same layer has the same behavior, and will end up having the same weight. This is called the symmetric problem.\n",
        "The same problem happens if the weights are initialized with a constant value for example all the weights are one and the biases are zero."
      ],
      "metadata": {
        "id": "6SsSdbYhHcSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate\n",
        "Learning rate affects the learning process of a neural network and eventually the accuracy of the model. In general three different cases may happen.\n",
        "\n",
        "- Correct learning rate\n",
        "- Too big learning rate\n",
        "- Too small learning rate\n",
        "\n",
        "In second case, our model may diverge and never find the optimal weight and biases. In the too small learning rate, the time it takes for the model to find the optimal results increases.\n",
        "\n"
      ],
      "metadata": {
        "id": "obyLPQ0GH1ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Best rate"
      ],
      "metadata": {
        "id": "P2JwNlryKlSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracies = {}\n",
        "\n",
        "lrs = [0.0005 + 0.0005*i for i in range(5)]\n",
        "\n",
        "for lr in lrs:\n",
        "  log = train_model(activation=Relu(), initial='uniform', print_results=False, lr=lr)\n",
        "  max_accuracies[lr] = {}\n",
        "  max_accuracies[lr]['train_accuracy'] = log['train_accuracy'][-1]\n",
        "  max_accuracies[lr]['test_accuracy'] = log['test_accuracy'][-1]"
      ],
      "metadata": {
        "id": "Z98sHmb-IDYX"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn0tFwxiRw92",
        "outputId": "2930c5e4-c58c-45e3-e59e-5a932ecf9113"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.0005: {'test_accuracy': 97.93754245923913,\n",
              "  'train_accuracy': 98.28424189136382},\n",
              " 0.001: {'test_accuracy': 98.5400390625, 'train_accuracy': 99.14883743649864},\n",
              " 0.0015: {'test_accuracy': 98.6572265625, 'train_accuracy': 99.41261234857366},\n",
              " 0.002: {'test_accuracy': 98.60648777173913,\n",
              "  'train_accuracy': 99.4284876905041},\n",
              " 0.0025: {'test_accuracy': 98.447265625, 'train_accuracy': 99.51519148104728}}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best accuracy seems to be 0.0015:"
      ],
      "metadata": {
        "id": "F3YMh6cwUJr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.0015"
      ],
      "metadata": {
        "id": "B3vZoRRxUkVB"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Small Rate\n"
      ],
      "metadata": {
        "id": "meO9cKvYKpmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "log = train_model(activation=Relu(), initial='uniform', print_results=True, lr=LEARNING_RATE * 0.1)\n",
        "t1 = time.time()\n",
        "print('Time = {}s'.format(t1 - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V02qftGfK38G",
        "outputId": "87065081-9ce8-41e8-fc01-1d7181aa7849"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\tTrain: Average Accuracy: 68.41471277842908\tAverage Loss: 0.9764623532463499\n",
            "\tTest: Average Accuracy: 90.76193104619566\tAverage Loss: 0.3267338043147354\n",
            "Epoch 2:\n",
            "\tTrain: Average Accuracy: 92.27115084017194\tAverage Loss: 0.26978816213334655\n",
            "\tTest: Average Accuracy: 94.08118206521739\tAverage Loss: 0.21846266752184557\n",
            "Epoch 3:\n",
            "\tTrain: Average Accuracy: 93.9417252833138\tAverage Loss: 0.21036962902968145\n",
            "\tTest: Average Accuracy: 94.90149456521739\tAverage Loss: 0.1868604932169852\n",
            "Epoch 4:\n",
            "\tTrain: Average Accuracy: 94.58284486127394\tAverage Loss: 0.1870424828922487\n",
            "\tTest: Average Accuracy: 95.31058933423913\tAverage Loss: 0.1706472351973587\n",
            "Epoch 5:\n",
            "\tTrain: Average Accuracy: 95.01880617428684\tAverage Loss: 0.17363544294290406\n",
            "\tTest: Average Accuracy: 95.58402683423913\tAverage Loss: 0.16067499592463613\n",
            "Epoch 6:\n",
            "\tTrain: Average Accuracy: 95.29601406799532\tAverage Loss: 0.16462625024575828\n",
            "\tTest: Average Accuracy: 95.76469089673913\tAverage Loss: 0.1539012020257148\n",
            "Epoch 7:\n",
            "\tTrain: Average Accuracy: 95.53780773739742\tAverage Loss: 0.15799413365074066\n",
            "\tTest: Average Accuracy: 95.89652683423913\tAverage Loss: 0.1489468625963725\n",
            "Epoch 8:\n",
            "\tTrain: Average Accuracy: 95.7038882375928\tAverage Loss: 0.15280099628322874\n",
            "\tTest: Average Accuracy: 95.97465183423913\tAverage Loss: 0.14511121298531604\n",
            "Epoch 9:\n",
            "\tTrain: Average Accuracy: 95.84920867526378\tAverage Loss: 0.14855712676038008\n",
            "\tTest: Average Accuracy: 96.10160495923913\tAverage Loss: 0.14202710753134906\n",
            "Epoch 10:\n",
            "\tTrain: Average Accuracy: 95.926143024619\tAverage Loss: 0.1449834736929914\n",
            "\tTest: Average Accuracy: 96.15043308423913\tAverage Loss: 0.1394698825506055\n",
            "Epoch 11:\n",
            "\tTrain: Average Accuracy: 96.03727041813208\tAverage Loss: 0.14190772940983298\n",
            "\tTest: Average Accuracy: 96.21879245923913\tAverage Loss: 0.13728716386064774\n",
            "Epoch 12:\n",
            "\tTrain: Average Accuracy: 96.13252246971473\tAverage Loss: 0.13920601751419776\n",
            "\tTest: Average Accuracy: 96.26762058423913\tAverage Loss: 0.1353969224553637\n",
            "Epoch 13:\n",
            "\tTrain: Average Accuracy: 96.20579327862447\tAverage Loss: 0.1368046863569388\n",
            "\tTest: Average Accuracy: 96.30668308423913\tAverage Loss: 0.13373635266392395\n",
            "Epoch 14:\n",
            "\tTrain: Average Accuracy: 96.27051582649473\tAverage Loss: 0.13464246058622942\n",
            "\tTest: Average Accuracy: 96.36039402173913\tAverage Loss: 0.13226669172087352\n",
            "Epoch 15:\n",
            "\tTrain: Average Accuracy: 96.32913247362251\tAverage Loss: 0.13268306745014635\n",
            "\tTest: Average Accuracy: 96.41898777173913\tAverage Loss: 0.13095090451032174\n",
            "Time = 359.01947689056396s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this rate the accuracy increases but the speed is extremly low thus the model needs more epochs to reach the same accuracy as the optimal learning rate."
      ],
      "metadata": {
        "id": "x9lXtCeHXtGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Big Rate"
      ],
      "metadata": {
        "id": "IC0Z6LIULHEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "log = train_model(activation=Relu(), initial='uniform', print_results=True, lr=LEARNING_RATE * 10)\n",
        "t1 = time.time()\n",
        "print('Time = {}s'.format(t1 - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxulBxVTLQqj",
        "outputId": "a7494b65-f586-4d16-ce5b-4ff5236e4ca2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\tTrain: Average Accuracy: 10.120945681906996\tAverage Loss: 2.331414180968476\n",
            "\tTest: Average Accuracy: 9.84566066576087\tAverage Loss: 2.306499681506468\n",
            "Epoch 2:\n",
            "\tTrain: Average Accuracy: 10.131936303243455\tAverage Loss: 2.306144628766946\n",
            "\tTest: Average Accuracy: 9.85054347826087\tAverage Loss: 2.306498360027534\n",
            "Epoch 3:\n",
            "\tTrain: Average Accuracy: 14.159876905041031\tAverage Loss: 2.196187483072227\n",
            "\tTest: Average Accuracy: 25.46556555706522\tAverage Loss: 1.9041259694644599\n",
            "Epoch 4:\n",
            "\tTrain: Average Accuracy: 19.709994138335286\tAverage Loss: 2.041700836373711\n",
            "\tTest: Average Accuracy: 19.04190726902174\tAverage Loss: 2.0239363294796284\n",
            "Epoch 5:\n",
            "\tTrain: Average Accuracy: 20.105656506447833\tAverage Loss: 2.0181559401989597\n",
            "\tTest: Average Accuracy: 19.73038383152174\tAverage Loss: 2.010495570989556\n",
            "Epoch 6:\n",
            "\tTrain: Average Accuracy: 20.191139116842518\tAverage Loss: 2.020078580683403\n",
            "\tTest: Average Accuracy: 20.29190726902174\tAverage Loss: 1.9946359557048363\n",
            "Epoch 7:\n",
            "\tTrain: Average Accuracy: 19.865084017194217\tAverage Loss: 2.025331788351169\n",
            "\tTest: Average Accuracy: 18.96378226902174\tAverage Loss: 2.0297853041084983\n",
            "Epoch 8:\n",
            "\tTrain: Average Accuracy: 19.974990230558813\tAverage Loss: 2.0267309569770173\n",
            "\tTest: Average Accuracy: 18.69522758152174\tAverage Loss: 2.0399896764580405\n",
            "Epoch 9:\n",
            "\tTrain: Average Accuracy: 19.802803829620945\tAverage Loss: 2.01997304186619\n",
            "\tTest: Average Accuracy: 19.06632133152174\tAverage Loss: 2.025734050768072\n",
            "Epoch 10:\n",
            "\tTrain: Average Accuracy: 19.89439234075811\tAverage Loss: 2.0165983255847064\n",
            "\tTest: Average Accuracy: 19.13468070652174\tAverage Loss: 2.0222351564154564\n",
            "Epoch 11:\n",
            "\tTrain: Average Accuracy: 19.72831184056272\tAverage Loss: 2.0237203875868\n",
            "\tTest: Average Accuracy: 19.15909476902174\tAverage Loss: 2.023965546987668\n",
            "Epoch 12:\n",
            "\tTrain: Average Accuracy: 20.346228995701445\tAverage Loss: 2.0065281732428297\n",
            "\tTest: Average Accuracy: 19.32511039402174\tAverage Loss: 2.0130669787751656\n",
            "Epoch 13:\n",
            "\tTrain: Average Accuracy: 22.409486127393514\tAverage Loss: 1.9612580583130554\n",
            "\tTest: Average Accuracy: 21.42747961956522\tAverage Loss: 2.0171994026501165\n",
            "Epoch 14:\n",
            "\tTrain: Average Accuracy: 29.941187964048456\tAverage Loss: 1.778259285527527\n",
            "\tTest: Average Accuracy: 33.09740149456522\tAverage Loss: 1.6952680383306469\n",
            "Epoch 15:\n",
            "\tTrain: Average Accuracy: 31.53028526768269\tAverage Loss: 1.7517103054066132\n",
            "\tTest: Average Accuracy: 32.91970957880435\tAverage Loss: 1.667142693696166\n",
            "Time = 350.45753145217896s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The networks accuracy doesn't change.\n",
        "This learning rate causes a problem called gradient vanishing."
      ],
      "metadata": {
        "id": "ac3Ft9rpXpUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Function\n",
        "In this part, different activation functions are tested on the network:"
      ],
      "metadata": {
        "id": "PrVC5PkcLfHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracies = {}\n",
        "\n",
        "activation_functions = {\n",
        "    'Sigmoid': Sigmoid(),\n",
        "    'Hyperbolic Tangent': Tanh(),\n",
        "    'Leaky Relu': LeakyRelu()\n",
        "}\n",
        "\n",
        "for activation in activation_functions:\n",
        "  t0 = time.time()\n",
        "  log = train_model(activation=activation_functions[activation], initial='uniform', print_results=False, lr=LEARNING_RATE)\n",
        "  t1 = time.time()\n",
        "  print('Time = {}s for {}'.format(t1 - t0, activation))\n",
        "  max_accuracies[activation] = {}\n",
        "  max_accuracies[activation]['train_accuracy'] = log['train_accuracy'][-1]\n",
        "  max_accuracies[activation]['test_accuracy'] = log['test_accuracy'][-1]  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4hP2FQbLhnm",
        "outputId": "016f1ad9-242f-4775-877b-d175e74e6537"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time = 346.8487813472748s for Sigmoid\n",
            "Time = 344.6701235771179s for Hyperbolic Tangent\n",
            "Time = 330.7944633960724s for Leaky Relu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g5vxK3Pb-7f",
        "outputId": "e602025d-80a4-4651-c334-fab6139e98b6"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Hyperbolic Tangent': {'test_accuracy': 90.00891644021739,\n",
              "  'train_accuracy': 90.02173700664322},\n",
              " 'Leaky Relu': {'test_accuracy': 98.544921875,\n",
              "  'train_accuracy': 99.38086166471278},\n",
              " 'Sigmoid': {'test_accuracy': 97.28812839673913,\n",
              "  'train_accuracy': 97.26455646737007}}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leaky Relu has the best performance and we'll use it for the rest of the assignment:"
      ],
      "metadata": {
        "id": "tD7sGGPhfjMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTIVATION = LeakyRelu()"
      ],
      "metadata": {
        "id": "fMtPBCSycJOp"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sigmoid function normalizes the values(all of them are between 0 and 1). But when x(exp power) is a large positive or negative value, it'll decrease to 0.\n",
        "Also, it's more computationally expensive than other functions.\n",
        "\n",
        "Hyperbolic Tangent function normalizes the values in between -1 and 1 thus it's a zero centered function.\n",
        "This property is important in deep learning because it has been empirically shown that models operating on normalized data have faster convergence. Unfortunately, zero-centered activation functions saturate at their asymptotes, leading to a weak training signal. ReLU avoids this problem but it is not zero-centered.\n",
        "\n",
        "The problem with Sigmoid and Tanhs' performance is that they saturate.\n",
        "A neuron is said to be saturated when extremely large weights cause the neuron to produce values (gradients) that are very close to the range boundary.\n",
        "If the gradient is constantly 0, no learning will take place in the neural network. Likewise, if the gradient is constantly 1, it most likely means that the neuron is over-fitting on training data and will likely perform poorly on test data.\n",
        "In these functions, over-fitting is happenning and the accuracy is relatively constant.\n",
        "\n",
        "Leaky Relu is faster both in convergence and its computation. This function doesn't saturate and the gradients don't get killed. But the problem is that it's not consistent.\n",
        "Leaky Relu has a slope for negative values while Relu deletes these values."
      ],
      "metadata": {
        "id": "ERJdI7e5YIUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Size\n",
        "The batch size defines the number of samples that will be propagated through the network.\n",
        "Batch size is one of the most important hyperparameters to tune in modern deep learning systems.\n",
        "This is because using the entire dataset guarantees convergence to the global optima of the objective function. However, this is at the cost of slower, empirical convergence to that optima.\n",
        "\n",
        "Now, let's try different batch sizes:"
      ],
      "metadata": {
        "id": "x9oPyWAUM8UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracies = {}\n",
        "\n",
        "sizes = [16, 256]\n",
        "\n",
        "for size in sizes:\n",
        "  log = train_model(activation=ACTIVATION, initial='uniform', print_results=False,\n",
        "                    lr=LEARNING_RATE, epochs=EPOCHS, batch_size=size)\n",
        "  max_accuracies[size] = {}\n",
        "  max_accuracies[size]['train_accuracy'] = log['train_accuracy'][-1]\n",
        "  max_accuracies[size]['test_accuracy'] = log['test_accuracy'][-1]"
      ],
      "metadata": {
        "id": "qOA9VS7fM_bv"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz2J7HaTfZqI",
        "outputId": "31fbea84-24b6-4188-adf8-ce01ec839f98"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{16: {'test_accuracy': 98.6083984375, 'train_accuracy': 99.46023837436499},\n",
              " 256: {'test_accuracy': 95.35288698760121, 'train_accuracy': 95.2763458111319}}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The larger batch size has poor performance. This is because with the larger batch size our model has poor generalization and over-fits and doesn't converge.\n",
        "It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. "
      ],
      "metadata": {
        "id": "fke_IEJbZaix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advantages of very small batch size\n",
        "It takes less memory.\n",
        "Also, it is more generalized and has better accuracy overall. Using smaller batch sizes have been empirically shown to have faster convergence to good solutions.\n",
        "This is intuitively explained by the fact that smaller batch sizes allow the model to start learning before having to see all the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "GmOhrJqHY45B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disadvantages of very small batch size\n",
        "The model is not guaranteed to converge to the global optima. It will bounce around the global optima, staying outside some -ball of the optima where  depends on the ratio of the batch size to the dataset size. Therefore, under no computational constraints, it is often advised that one starts at a small batch size, reaping the benefits of faster training dynamics, and steadily grows the batch size through training, also reaping the benefits of guaranteed convergence."
      ],
      "metadata": {
        "id": "lV6MN8PAZANx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "Artificial neural networks have been pretty successful in predicting various mechanical properties of fibre reinforced composites. Most studies have stressed that the number of training datasets plays a key role in ANN predictive quality. For more complex nonlinear relations between input and output, larger training datasets were found to be more successful."
      ],
      "metadata": {
        "id": "1c1mHtOiNyrR"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "AI-CA5-P1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}